{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02-BOW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we master the preprocessing, let's make our first Bag Of Words (BOW)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will reuse our dataset of Coldplay songs to make a BOW."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As usual, the first step is to import some libraries. So import *nltk* as well as all the libraries you will need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import NLTK and all the needed libraries\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load now the dataset in *coldplay.csv* using pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Artist                           Song  \\\n",
      "0  Coldplay                 Another's Arms   \n",
      "1  Coldplay                Bigger Stronger   \n",
      "2  Coldplay                       Daylight   \n",
      "3  Coldplay                       Everglow   \n",
      "4  Coldplay  Every Teardrop Is A Waterfall   \n",
      "\n",
      "                                                Link  \\\n",
      "0            /c/coldplay/anothers+arms_21079526.html   \n",
      "1          /c/coldplay/bigger+stronger_20032648.html   \n",
      "2                 /c/coldplay/daylight_20032625.html   \n",
      "3                 /c/coldplay/everglow_21104546.html   \n",
      "4  /c/coldplay/every+teardrop+is+a+waterfall_2091...   \n",
      "\n",
      "                                              Lyrics  \n",
      "0  Late night watching tv  \\nUsed to be you here ...  \n",
      "1  I want to be bigger stronger drive a faster ca...  \n",
      "2  To my surprise, and my delight  \\nI saw sunris...  \n",
      "3  Oh, they say people come  \\nThey say people go...  \n",
      "4  I turn the music up, I got my records on  \\nI ...  \n"
     ]
    }
   ],
   "source": [
    "# TODO: Load the dataset in coldplay.csv\n",
    "# Load the dataset\n",
    "data = pd.read_csv('coldplay.csv')\n",
    "\n",
    "# Print the first 5 rows of the dataset to check if it has been loaded correctly\n",
    "print(data.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You already know this dataset, but you can check it again if you want to refresh your memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 120 entries, 0 to 119\n",
      "Data columns (total 4 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   Artist  120 non-null    object\n",
      " 1   Song    120 non-null    object\n",
      " 2   Link    120 non-null    object\n",
      " 3   Lyrics  120 non-null    object\n",
      "dtypes: object(4)\n",
      "memory usage: 3.9+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# TODO: Explore the data\n",
    "import pandas as pd\n",
    "data = pd.read_csv('coldplay.csv')\n",
    "print(data.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now using the *CountVectorizer* of scikit-learn, make a BOW of all the lyrics of Coldplay, and print the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'arm': 18, 'anoth': 16, 'bodi': 4, 'late': 3, 'night': 3, 'watch': 3, 'tv': 3, 'use': 3, 'besid': 3, 'right': 3, 'pull': 3, 'around': 2, 'world': 2, 'mean': 2, 'noth': 2, 'pain': 2, 'rip': 2, 'someon': 2, 'wish': 2, 'reach': 1, 'find': 1, 'tortur': 1, 'got': 1, 'close': 1})\n"
     ]
    }
   ],
   "source": [
    "# TODO: Compute a BOW\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file into a Pandas DataFrame\n",
    "df = pd.read_csv('coldplay.csv')\n",
    "\n",
    "# Access the lyrics of the first song\n",
    "lyrics = df.loc[0, 'Lyrics']\n",
    "\n",
    "# Tokenize the lyrics\n",
    "tokens = nltk.word_tokenize(lyrics)\n",
    "\n",
    "# Remove the punctuation\n",
    "tokens = [token.lower() for token in tokens if token.isalnum()]\n",
    "\n",
    "# Remove the stop words\n",
    "stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "tokens = [token for token in tokens if token not in stop_words]\n",
    "\n",
    "# Lemmatize the tokens\n",
    "lemmatizer = nltk.WordNetLemmatizer()\n",
    "tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "# Perform stemming\n",
    "stemmer = nltk.PorterStemmer()\n",
    "tokens = [stemmer.stem(token) for token in tokens]\n",
    "\n",
    "# Store the preprocessed text in the preprocessed_text variable\n",
    "preprocessed_text = tokens\n",
    "\n",
    "\n",
    "# Compute the frequency of each token in the preprocessed text\n",
    "word_counts = Counter(preprocessed_text)\n",
    "\n",
    "# Print the word counts\n",
    "print(word_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the BOW matrix, we would like to have a new dataframe having the BOW for each song, and as columns the corresponding words (just as we did in the lecture at the end).\n",
    "\n",
    "So that at the end we would end up with a dataframe containing something like the following (120 raws for 120 songs, and as many columns as words):\n",
    "\n",
    "| | ah | adventure | ... | yeah \n",
    "|---|---|---|---|---| \n",
    "| 0 | 0 | 1 | ... | 4 |\n",
    "| 1 | 8 | 0 | ... | 2 |\n",
    "|...|...|...|...|...|\n",
    "| 119 | 5 | 0 | ... | 8 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Word  Count\n",
      "0     late      3\n",
      "1    night      3\n",
      "2    watch      3\n",
      "3       tv      3\n",
      "4      use      3\n",
      "5    besid      3\n",
      "6      arm     18\n",
      "7   around      2\n",
      "8     bodi      4\n",
      "9    world      2\n",
      "10    mean      2\n",
      "11    noth      2\n",
      "12   anoth     16\n",
      "13    pain      2\n",
      "14     rip      2\n",
      "15   right      3\n",
      "16  someon      2\n",
      "17   reach      1\n",
      "18    find      1\n",
      "19  tortur      1\n",
      "20    pull      3\n",
      "21     got      1\n",
      "22   close      1\n",
      "23    wish      2\n"
     ]
    }
   ],
   "source": [
    "# TODO: Create a new dataframe containing the BOW outputs and the corresponding words as columns. And print it\n",
    "bow_df = pd.DataFrame(list(word_counts.items()), columns=['Word', 'Count'])\n",
    "print(bow_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well as you see we're still having some issue, we have some tokens that are not words, like '10' or '2000'.\n",
    "\n",
    "To get rid of that, we could use directly regular expressions within the function. Another solution would be to make preprocessing before using the function *CountVectorizer*.\n",
    "\n",
    "For the moment, we won't pay attention to this issue. But if you are curious and have time, you can find on google how to remove those words using the *CountVectorizer*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we would like to see what are the most used words by Coldplay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'oh'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum_bow = bow_df.sum()\n",
    "sum_bow.idxmax()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So what is the most used word? Are you surprised?\n",
    "\n",
    "Now make a sort in order to show the 10 most used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('arm', 18), ('anoth', 16), ('bodi', 4), ('late', 3), ('night', 3), ('watch', 3), ('tv', 3), ('use', 3), ('besid', 3), ('right', 3)]\n"
     ]
    }
   ],
   "source": [
    "# TODO: print the 10 most used word by Coldplay\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file into a Pandas DataFrame\n",
    "df = pd.read_csv('coldplay.csv')\n",
    "\n",
    "# Access the lyrics of the first song\n",
    "lyrics = df.loc[0, 'Lyrics']\n",
    "\n",
    "# Tokenize the lyrics\n",
    "tokens = nltk.word_tokenize(lyrics)\n",
    "\n",
    "# Remove the punctuation\n",
    "tokens = [token.lower() for token in tokens if token.isalnum()]\n",
    "\n",
    "# Remove the stop words\n",
    "stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "tokens = [token for token in tokens if token not in stop_words]\n",
    "\n",
    "# Lemmatize the tokens\n",
    "lemmatizer = nltk.WordNetLemmatizer()\n",
    "tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "# Perform stemming\n",
    "stemmer = nltk.PorterStemmer()\n",
    "tokens = [stemmer.stem(token) for token in tokens]\n",
    "\n",
    "# Store the preprocessed text in the preprocessed_text variable\n",
    "preprocessed_text = tokens\n",
    "\n",
    "\n",
    "# Compute the frequency of each token in the preprocessed text\n",
    "word_counts = Counter(preprocessed_text)\n",
    "\n",
    "# Print the 10 most used words by Coldplay\n",
    "print(word_counts.most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here it is! You know the Coldplay lyrics more than the singers now!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
