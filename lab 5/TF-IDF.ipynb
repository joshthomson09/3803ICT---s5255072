{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01-TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will here compute the TF-IDF on a corpus of newspaper headlines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Begin by importing needed libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import needed libraries\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the data into the file *headlines.csv*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Load the dataset\n",
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('headlines.csv')\n",
    "\n",
    "# Display the first 5 rows of the dataframe\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As usual, check the dataset basic information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   publish_date                                      headline_text\n",
      "0      20170721  algorithms can make decisions on behalf of fed...\n",
      "1      20170721  andrew forrests fmg to appeal pilbara native t...\n",
      "2      20170721                           a rural mural in thallan\n",
      "3      20170721  australia church risks becoming haven for abusers\n",
      "4      20170721  australian company usgfx embroiled in shanghai...\n",
      "5      20170721  australia suffers shock loss in womens world c...\n",
      "6      20170721                                           big rigs\n",
      "7      20170721  boy charged in connection with supermarket syr...\n",
      "8      20170721  breaking bad creator vince gilligan on success...\n",
      "9      20170721  breaking bad creator vince gilligan on walter ...\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1999 entries, 0 to 1998\n",
      "Data columns (total 2 columns):\n",
      " #   Column         Non-Null Count  Dtype \n",
      "---  ------         --------------  ----- \n",
      " 0   publish_date   1999 non-null   int64 \n",
      " 1   headline_text  1999 non-null   object\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 31.4+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('headlines.csv')\n",
    "\n",
    "# Print the first 10 rows\n",
    "print(df.head(10))\n",
    "\n",
    "# Print some basic information about the dataset\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now perform preprocessing on this text data: tokenization, punctuation and stop words removal and stemming.\n",
    "\n",
    "Hint: to do so, use NLTK, *pandas*'s method *apply*, lambda functions and list comprehension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "import string\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(\"headlines.csv\")\n",
    "\n",
    "# Tokenize the text\n",
    "df['tokens'] = df['headline_text'].apply(lambda x: word_tokenize(x))\n",
    "\n",
    "# Remove punctuation\n",
    "punctuations = string.punctuation\n",
    "df['tokens'] = df['tokens'].apply(lambda x: [word for word in x if word not in punctuations])\n",
    "\n",
    "# Remove stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "df['tokens'] = df['tokens'].apply(lambda x: [word for word in x if word.lower() not in stop_words])\n",
    "\n",
    "# Stem the words\n",
    "stemmer = SnowballStemmer('english')\n",
    "df['tokens'] = df['tokens'].apply(lambda x: [stemmer.stem(word) for word in x])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute now the Bag of Words for our data, using scikit-learn.\n",
    "\n",
    "Warning: since we used our own preprocessing, you have to bypass analyzer with identity function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/joshuathomson/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/joshuathomson/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/joshuathomson/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/joshuathomson/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 4156\n"
     ]
    }
   ],
   "source": [
    "import ssl\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "\n",
    "import nltk\n",
    "import os\n",
    "\n",
    "# Set the data path to a directory where you have write permission\n",
    "nltk.data.path.append(os.getcwd())\n",
    "\n",
    "# Download the required data\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Load the dataset\n",
    "import pandas as pd\n",
    "df = pd.read_csv('headlines.csv')\n",
    "\n",
    "# Preprocess the headlines\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "preprocessed_headlines = df['headline_text'].apply(lambda x: ' '.join([stemmer.stem(w) for w in word_tokenize(x) if w not in stop_words and w.isalpha()]))\n",
    "\n",
    "# Apply BOW transformation\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "bow = vectorizer.fit_transform(preprocessed_headlines)\n",
    "\n",
    "# Print the vocabulary size\n",
    "print(\"Vocabulary size:\", len(vectorizer.vocabulary_))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can check the shape of the BOW, the expected value is `(1999, 4165)`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now compute the Term Frequency and then the Inverse Document Frequency, and check the values are not only zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'first': 1, 'document': 1, '.': 1}, {'second': 1, 'document': 1, '.': 1}, {'third': 1, 'document': 1, '.': 1}, {'first': 1, 'document': 1, '?': 1}]\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Define the compute_tf function\n",
    "def compute_tf(documents):\n",
    "    tf_matrix = []\n",
    "    for doc in documents:\n",
    "        # Tokenize the document\n",
    "        tokens = word_tokenize(doc.lower())\n",
    "\n",
    "        # Remove stop words\n",
    "        filtered_tokens = [token for token in tokens if token not in stopwords.words('english')]\n",
    "\n",
    "        # Compute the term frequency\n",
    "        tf = {}\n",
    "        for token in filtered_tokens:\n",
    "            tf[token] = tf.get(token, 0) + 1\n",
    "        tf_matrix.append(tf)\n",
    "\n",
    "    return tf_matrix\n",
    "\n",
    "# Define the documents\n",
    "documents = [\n",
    "    'This is the first document.',\n",
    "    'This is the second document.',\n",
    "    'And this is the third document.',\n",
    "    'Is this the first document?'\n",
    "]\n",
    "\n",
    "# Compute the term frequency matrix\n",
    "tf_matrix = compute_tf(documents)\n",
    "\n",
    "# Print the results\n",
    "print(tf_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/joshuathomson/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/joshuathomson/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/joshuathomson/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['algorithm', 'make', 'decision', 'behalf', 'federal', 'minister'], ['andrew', 'forrests', 'fmg', 'appeal', 'pilbara', 'native', 'title', 'ruling'], ['rural', 'mural', 'thallan'], ['australia', 'church', 'risk', 'becoming', 'abuser'], ['australian', 'company', 'usgfx', 'embroiled', 'shanghai', 'staff', 'standoff'], ['australia', 'suffers', 'shock', 'loss', 'woman', 'world', 'cup', 'semi'], ['big', 'rig'], ['boy', 'charged', 'connection', 'supermarket', 'syringe', 'incident'], ['breaking', 'bad', 'creator', 'vince', 'gilligan', 'success'], ['breaking', 'bad', 'creator', 'vince', 'gilligan', 'walter', 'white', 'tv']]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "import string\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv('headlines.csv', usecols=['headline_text'])\n",
    "documents = df['headline_text'].tolist()\n",
    "\n",
    "# Pre-processing\n",
    "def preprocess(documents):\n",
    "    # Remove punctuation\n",
    "    documents = [re.sub(r'[^\\w\\s]', '', doc) for doc in documents]\n",
    "\n",
    "    # Tokenize\n",
    "    tokenized_docs = [word_tokenize(doc.lower()) for doc in documents]\n",
    "\n",
    "    # Remove stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_docs = [[word for word in doc if not word in stop_words] for doc in tokenized_docs]\n",
    "\n",
    "    # Lemmatize\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_docs = [[lemmatizer.lemmatize(word) for word in doc] for doc in filtered_docs]\n",
    "\n",
    "    return lemmatized_docs\n",
    "\n",
    "preprocessed_docs = preprocess(documents)\n",
    "print(preprocessed_docs[:10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute finally the TF-IDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'this': 8, 'is': 3, 'the': 6, 'first': 2, 'document': 1, 'second': 5, 'and': 0, 'third': 7, 'one': 4}\n",
      "[1.91629073 1.22314355 1.51082562 1.         1.91629073 1.91629073\n",
      " 1.         1.91629073 1.        ]\n",
      "[[0.         0.46979139 0.58028582 0.38408524 0.         0.\n",
      "  0.38408524 0.         0.38408524]\n",
      " [0.         0.6876236  0.         0.28108867 0.         0.53864762\n",
      "  0.28108867 0.         0.28108867]\n",
      " [0.51184851 0.         0.         0.26710379 0.51184851 0.\n",
      "  0.26710379 0.51184851 0.26710379]\n",
      " [0.         0.46979139 0.58028582 0.38408524 0.         0.\n",
      "  0.38408524 0.         0.38408524]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# example corpus of documents\n",
    "corpus = [\n",
    "    \"This is the first document.\",\n",
    "    \"This document is the second document.\",\n",
    "    \"And this is the third one.\",\n",
    "    \"Is this the first document?\",\n",
    "]\n",
    "\n",
    "# create TF-IDF vectorizer object\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# fit and transform the corpus\n",
    "tf_idf = vectorizer.fit_transform(corpus)\n",
    "\n",
    "# print the vocabulary\n",
    "print(vectorizer.vocabulary_)\n",
    "\n",
    "# print the IDF scores\n",
    "print(vectorizer.idf_)\n",
    "\n",
    "# print the TF-IDF matrix\n",
    "print(tf_idf.toarray())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the 10 words with the highest and lowest TF-IDF on average?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 words by average TF-IDF:\n",
      "is             : 0.3797\n",
      "the            : 0.3531\n",
      "first          : 0.2422\n",
      "and            : 0.2422\n",
      "one            : 0.2422\n",
      "second         : 0.1820\n",
      "this           : 0.1393\n",
      "document       : 0.1393\n",
      "third          : 0.1393\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 25\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTop 10 words by average TF-IDF:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m10\u001b[39m):\n\u001b[0;32m---> 25\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{:<15}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{:.4f}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[43mtfidf_avg\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m[\u001b[38;5;241m0\u001b[39m], tfidf_avg[i][\u001b[38;5;241m1\u001b[39m]))\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# print the 10 words with the lowest tf-idf average\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mBottom 10 words by average TF-IDF:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "# create the vectorizer and transformer\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "\n",
    "# compute the term frequency matrix\n",
    "counts = vectorizer.fit_transform(corpus)\n",
    "\n",
    "# compute the tf-idf matrix\n",
    "tfidf = tfidf_transformer.fit_transform(counts)\n",
    "\n",
    "# create a list of (word, tf-idf average) pairs\n",
    "tfidf_avg = []\n",
    "for i, word in enumerate(vectorizer.vocabulary_):\n",
    "    tfidf_avg.append((word, np.mean(tfidf[:, i].toarray())))\n",
    "\n",
    "# sort the list by tf-idf in descending order\n",
    "tfidf_avg = sorted(tfidf_avg, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# print the 10 words with the highest tf-idf average\n",
    "print(\"Top 10 words by average TF-IDF:\")\n",
    "for i in range(10):\n",
    "    print(\"{:<15}: {:.4f}\".format(tfidf_avg[i][0], tfidf_avg[i][1]))\n",
    "\n",
    "# print the 10 words with the lowest tf-idf average\n",
    "print(\"\\nBottom 10 words by average TF-IDF:\")\n",
    "for i in range(-1, -11, -1):\n",
    "    print(\"{:<15}: {:.4f}\".format(tfidf_avg[i][0], tfidf_avg[i][1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's compute the TF-IDF using scikit-learn on our preprocessed data (the one you used to compute the BOW)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import the module\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Instantiate the TF-IDF vectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Compute the TF-IDF\n",
    "tfidf = vectorizer.fit_transform(corpus)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the 10 highest and lowest TF-IDF words on average to the ones you had by yourself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 words with highest TF-IDF on average:\n",
      "is: 0.3797333352764771\n",
      "the: 0.35307900317259394\n",
      "first: 0.2421757743247982\n",
      "and: 0.2421757743247982\n",
      "one: 0.2421757743247982\n",
      "second: 0.18202144022735134\n",
      "this: 0.13926610266483697\n",
      "document: 0.13926610266483697\n",
      "third: 0.13926610266483697\n",
      "\n",
      "Top 10 words with lowest TF-IDF on average:\n",
      "is: 0.3797333352764771\n",
      "the: 0.35307900317259394\n",
      "first: 0.2421757743247982\n",
      "and: 0.2421757743247982\n",
      "one: 0.2421757743247982\n",
      "second: 0.18202144022735134\n",
      "this: 0.13926610266483697\n",
      "document: 0.13926610266483697\n",
      "third: 0.13926610266483697\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# create the corpus\n",
    "corpus = [\n",
    "    'This is the first document.',\n",
    "    'This document is the second document.',\n",
    "    'And this is the third one.',\n",
    "    'Is this the first document?',\n",
    "]\n",
    "\n",
    "# create the vectorizer and transformer\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "\n",
    "# compute the term frequency matrix\n",
    "counts = vectorizer.fit_transform(corpus)\n",
    "\n",
    "# compute the tf-idf matrix\n",
    "tfidf = tfidf_transformer.fit_transform(counts)\n",
    "\n",
    "# compute the average tf-idf scores for each word\n",
    "tfidf_avg = []\n",
    "for i, word in enumerate(vectorizer.vocabulary_):\n",
    "    tfidf_avg.append((word, np.mean(tfidf[:, i].toarray())))\n",
    "\n",
    "# sort the list by tf-idf in descending order\n",
    "tfidf_avg = sorted(tfidf_avg, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# print the 10 words with the highest tf-idf on average\n",
    "print(\"Top 10 words with highest TF-IDF on average:\")\n",
    "for word, score in tfidf_avg[:10]:\n",
    "    print(f\"{word}: {score}\")\n",
    "\n",
    "# print the 10 words with the lowest tf-idf on average\n",
    "print(\"\\nTop 10 words with lowest TF-IDF on average:\")\n",
    "for word, score in tfidf_avg[-10:]:\n",
    "    print(f\"{word}: {score}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do you have the same words? How do you explain it?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
